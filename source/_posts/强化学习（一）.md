---
title: 强化学习（一）
date: 2022-10-17 15:42:18
tags: [强化学习, python]
excerpt: 内容来源于《动手学强化学习》
categories: 强化学习
index_img: /img/index_img/9.png
banner_img: /img/banner_img/background9.jpg

---

<a class="btn" target="_blank" rel="noopener" style="font-size:20px; color: green" href="https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0" title="github">动手学强化学习</a>

## 什么是强化学习

>广泛地讲，强化学习是机器通过与环境交互来实现目标的一种计算方法。机器和环境的一轮交互是指，机器在环境的一个状态下做一个动作决策，把这个动作作用到环境当中，这个环境发生相应的改变并且将相应的奖励反馈和下一轮状态传回机器。这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中获得的累积奖励的期望。强化学习用智能体（agent）这个概念来表示做决策的机器。相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号。

![](https://raw.githubusercontent.com/univwang/img/main/20221017155450.png)

## 多臂老虎机问题

### 形式化描述

假设每个时间步只能拉动一个拉杆，多臂老虎机的目标为最大化一段时间步$T$内累积的奖励: $\max \sum_{t=1}^{T} r_{t}, r_{t} \sim \mathcal{R}\left(\cdot \mid a_{t}\right)$。
其中$a_t$表示在第时间步拉动某一拉杆的动作，$r_t$表示动作$a_t$获得的奖励。


### 积累懊悔

懊悔定义为拉动当前拉杆的动作与最优拉杆的期望奖励差，即$R(a) = Q^* - Q(a)$
MAB 问题的目标为最大化累积奖励，等价于最小化累积懊悔。

### 估计期望奖励

为了知道拉动哪一根拉杆能获得更高的奖励，我们需要估计拉动这根拉杆的期望奖励。由于只拉动一次拉杆获得的奖励存在随机性，所以需要多次拉动一根拉杆，然后计算得到的多次奖励的期望，其算法流程如下所示。

更新公式如下

$\hat{Q}\left(a_{t}\right)=\hat{Q}\left(a_{t}\right)+\frac{1}{N\left(a_{t}\right)}\left[r_{t}-\hat{Q}\left(a_{t}\right)\right]$

### $\epsilon$-贪心算法

完全贪婪算法即在每一时刻采取期望奖励估值最大的动作（拉动拉杆），这就是纯粹的利用，而没有探索，所以我们通常需要对完全贪婪算法进行一些修改，其中比较经典的一种方法为 $\epsilon$-贪婪（$\epsilon$-Greedy）算法。$\epsilon$-贪婪算法在完全贪婪算法的基础上添加了噪声，每次以概率$\epsilon$选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率$1 - \epsilon$随机选择一根拉杆（探索）


## 马尔可夫决策过程

**马尔可夫决策过程**（Markov decision process，MDP）是强化学习的重要概念。要学好强化学习，我们首先要掌握马尔可夫决策过程的基础知识。前两章所说的强化学习中的环境一般就是一个马尔可夫决策过程。与多臂老虎机问题不同，马尔可夫决策过程包含状态信息以及状态之间的转移机制。如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。本章将从马尔可夫过程出发，一步一步地进行介绍，最后引出马尔可夫决策过程。


## Q-Learning


$Q\left(s_{t}, a_{t}\right) \leftarrow Q\left(s_{t}, a_{t}\right)+\alpha\left[R_{t}+\gamma \max _{a} Q\left(s_{t+1}, a\right)-Q\left(s_{t}, a_{t}\right)\right]$


具体流程

![](https://raw.githubusercontent.com/univwang/img/main/20221017162013.png)


我们可以用价值迭代的思想来理解 Q-learning，即 Q-learning 是直接在估计$Q^*$，因为动作价值函数的贝尔曼最优方程是$Q^{*}(s, a)=r(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)$