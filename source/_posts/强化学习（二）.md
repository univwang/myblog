---
title: 强化学习（二）
date: 2022-10-17 16:42:18
tags: [强化学习, python]
excerpt: DQN算法
categories: 强化学习
index_img: /img/index_img/10.png
banner_img: /img/banner_img/background10.jpg
---

<a class="btn" target="_blank" rel="noopener" style="font-size:20px; color: green" href="https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0" title="github">动手学强化学习</a>



## 简介

在第 5 章讲解的 Q-learning 算法中，我们以矩阵的方式建立了一张存储每个状态下所有动作$Q$值的表格。表格中的每一个动作价值$Q(s,a)$表示在状态$s$下选择动作$a$然后继续遵循某一策略预期能够得到的期望回报。当状态或者动作数量非常大的时候，我们需要用函数拟合的方法来估计$Q$值

## DQN
![](https://hrl.boyuai.com/static/cartpole.e4a03ca5.gif)


现在我们想在类似车杆的环境中得到动作价值函数$Q(s,a)$，由于状态每一维度的值都是连续的，无法使用表格记录，因此一个常见的解决方法便是使用函数拟合（function approximation）的思想。由于神经网络具有强大的表达能力，因此我们可以用一个神经网络来表示函数$Q$。若动作是连续（无限）的，神经网络的输入是状态$s$和动作$a$，然后输出一个标量，表示在状态$s$下采取动作$a$能获得的价值。若动作是离散（有限）的，除了可以采取动作连续情况下的做法，我们还可以只将$s$状态输入到神经网络中，使其同时输出每一个动作的$Q$值。通常 DQN（以及 Q-learning）只能处理动作离散的情况，因为在函数$Q$的更新过程中有$max_a$这一操作。

针对一组数据${(s_i​,a_i​,r_i​,s_i^{'}​)}$我们可以很自然地将 $Q$ 网络的损失函数构造为均方误差的形式

$$\omega^{*}=\arg \min _{\omega} \frac{1}{2 N} \sum_{i=1}^{N}\left[Q_{\omega}\left(s_{i}, a_{i}\right)-\left(r_{i}+\gamma \max _{a^{\prime}} Q_{\omega}\left(s_{i}^{\prime}, a^{\prime}\right)\right)\right]^{2}$$


至此，我们就可以将 Q-learning 扩展到神经网络形式——深度 Q 网络（deep Q network，DQN）算法。由于 DQN 是离线策略算法，因此我们在收集数据的时候可以使用一个$\epsilon$-贪婪策略来平衡探索与利用，将收集到的数据存储起来，在后续的训练中使用。DQN 中还有两个非常重要的模块——经验回放和目标网络，它们能够帮助 DQN 取得稳定、出色的性能。


## 经验回放

## 目标网络
